import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from torch.utils.data import DataLoader, TensorDataset
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm  # Import tqdm for progress bar

# 1. Load the data
file_path = "ohlcv.csv"  # Replace with your file path
data = pd.read_csv(file_path)

# Ensure datetime is parsed correctly
data['datetime'] = pd.to_datetime(data['datetime'])

# 2. Feature Engineering: Add Moving Averages
data['MA_3'] = data['low'].rolling(window=3).mean()  # 3-period moving average
data['MA_5'] = data['low'].rolling(window=5).mean()  # 5-period moving average
data.fillna(method='bfill', inplace=True)  # Fill NaN values
'''
if possible, need to add bollingerband or RSI, ATR(Supertrend)
'''

# Normalize features
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data[['open', 'high', 'low', 'close', 'volume', 'MA_3', 'MA_5']])

# 3. Create input features and target
sequence_length = 5  # Use the last 5 rows to predict the next
'''
for optimize, but not overfitting, need to adjust this section.
'''
X, y = [], []
for i in range(sequence_length, len(scaled_data)):
    X.append(scaled_data[i-sequence_length:i])  # Last 5 rows
    y.append(1 if scaled_data[i, 2] > scaled_data[i-1, 2] else 0)  # 1 if next 'low' > current 'low', else 0

X, y = np.array(X), np.array(y)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

# Batch processing with DataLoader
batch_size = 64
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# 4. Define the LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        _, (hidden, _) = self.lstm(x)  # Only use the hidden state
        out = self.fc(hidden[-1])  # Pass the last hidden state to the fully connected layer
        return out

# Initialize the model
input_size = 7  # Number of features: open, high, low, close, volume, MA_3, MA_5
hidden_size = 40  # Reduced hidden size to save memory
output_size = 2  # Predict: 0 (low down) or 1 (low up)
model = LSTMModel(input_size, hidden_size, output_size)

# 5. Train the model
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 2000
losses = []  # For tracking loss

for epoch in range(epochs):
    model.train()
    epoch_loss = 0
    with tqdm(total=len(train_loader), desc=f"Epoch {epoch+1}/{epochs}", unit="batch") as pbar:
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            pbar.set_postfix({"Batch Loss": loss.item()})
            pbar.update(1)  # Update the progress bar
    
    average_loss = epoch_loss / len(train_loader)
    losses.append(average_loss)
    print(f"Epoch {epoch+1}/{epochs}, Average Loss: {average_loss:.4f}")

# Plot training loss
plt.figure(figsize=(10, 5))
plt.plot(losses, label="Training Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss Over Epochs")
plt.legend()
plt.show()

# 6. Evaluate the model
model.eval()
with torch.no_grad():
    test_outputs = model(X_test)
    _, predictions = torch.max(test_outputs, 1)
    accuracy = (predictions == y_test).sum().item() / y_test.size(0)
    print(f"Test Accuracy: {accuracy * 100:.2f}%")

    # Classification report
    print("Classification Report:")
    print(classification_report(y_test, predictions, target_names=["Low Down", "Low Up"]))

    # Confusion Matrix
    cm = confusion_matrix(y_test, predictions)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["Low Down", "Low Up"], yticklabels=["Low Down", "Low Up"])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

# 7. Visualize predictions vs actual movements
plt.figure(figsize=(15, 5))
plt.plot(y_test.numpy(), label="Actual", alpha=0.7)
plt.plot(predictions.numpy(), label="Predicted", alpha=0.7)
plt.legend()
plt.title("Predicted vs Actual Low Movements")
plt.xlabel("Test Data Index")
plt.ylabel("Movement (0: Down, 1: Up)")
plt.show()
